{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Shian/Programs/Python/Statistical-Machine-Learning-Project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/train.csv', 'rb') as csvfile:\n",
    "    training_data = np.loadtxt(csvfile, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perm_data = np.random.permutation(training_data)\n",
    "n_samples = len(perm_data)\n",
    "\n",
    "img_rows, img_cols = 33, 13\n",
    "\n",
    "data = perm_data[:, 9:439].reshape(n_samples, 1, img_rows, img_cols)\n",
    "target = perm_data[:, 1].astype(int)\n",
    "\n",
    "input_shape = (1, img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_classes = 98\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 33, 13\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "X_data = data.astype('float32')\n",
    "X_data /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random image generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_target = np_utils.to_categorical(target, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "batch_size = 128\n",
    "nb_filters = 32\n",
    "nb_epoch = 200\n",
    "\n",
    "# Construct model, structure similar to LeNet5 from MNIST author\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Adadelta is some kind of adaptive learning rate optimiser, was just in the code I copied\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "50282/50282 [==============================] - 80s - loss: 2.5349 - acc: 0.4154 - val_loss: 1.9920 - val_acc: 0.5672\n",
      "Epoch 2/200\n",
      "50282/50282 [==============================] - 76s - loss: 2.1109 - acc: 0.5503 - val_loss: 1.8364 - val_acc: 0.6185\n",
      "Epoch 3/200\n",
      "50282/50282 [==============================] - 77s - loss: 2.0096 - acc: 0.5774 - val_loss: 1.7946 - val_acc: 0.6207\n",
      "Epoch 4/200\n",
      "50282/50282 [==============================] - 76s - loss: 1.9589 - acc: 0.5918 - val_loss: 1.7577 - val_acc: 0.6249\n",
      "Epoch 5/200\n",
      "50282/50282 [==============================] - 77s - loss: 1.9188 - acc: 0.6043 - val_loss: 1.7620 - val_acc: 0.6279\n",
      "Epoch 6/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.8865 - acc: 0.6117 - val_loss: 2.0962 - val_acc: 0.5000\n",
      "Epoch 7/200\n",
      "50282/50282 [==============================] - 77s - loss: 1.8627 - acc: 0.6197 - val_loss: 1.6752 - val_acc: 0.6484\n",
      "Epoch 8/200\n",
      "50282/50282 [==============================] - 77s - loss: 1.8479 - acc: 0.6228 - val_loss: 1.7178 - val_acc: 0.6372\n",
      "Epoch 9/200\n",
      "50282/50282 [==============================] - 78s - loss: 1.8335 - acc: 0.6271 - val_loss: 1.6739 - val_acc: 0.6464\n",
      "Epoch 10/200\n",
      "50282/50282 [==============================] - 77s - loss: 1.8149 - acc: 0.6294 - val_loss: 1.6568 - val_acc: 0.6573\n",
      "Epoch 11/200\n",
      "50282/50282 [==============================] - 77s - loss: 1.8055 - acc: 0.6325 - val_loss: 1.6493 - val_acc: 0.6571\n",
      "Epoch 12/200\n",
      "50282/50282 [==============================] - 77s - loss: 1.7993 - acc: 0.6361 - val_loss: 1.6135 - val_acc: 0.6671\n",
      "Epoch 13/200\n",
      "50282/50282 [==============================] - 78s - loss: 1.7811 - acc: 0.6377 - val_loss: 1.6840 - val_acc: 0.6432\n",
      "Epoch 14/200\n",
      "50282/50282 [==============================] - 76s - loss: 1.7780 - acc: 0.6394 - val_loss: 1.5804 - val_acc: 0.6702\n",
      "Epoch 15/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7668 - acc: 0.6431 - val_loss: 1.6079 - val_acc: 0.6615\n",
      "Epoch 16/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7599 - acc: 0.6418 - val_loss: 1.5715 - val_acc: 0.6718\n",
      "Epoch 17/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7529 - acc: 0.6462 - val_loss: 1.6208 - val_acc: 0.6613\n",
      "Epoch 18/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7498 - acc: 0.6459 - val_loss: 1.6068 - val_acc: 0.6665\n",
      "Epoch 19/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7440 - acc: 0.6474 - val_loss: 1.5952 - val_acc: 0.6659\n",
      "Epoch 20/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7301 - acc: 0.6502 - val_loss: 1.5741 - val_acc: 0.6718\n",
      "Epoch 21/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7348 - acc: 0.6495 - val_loss: 1.5857 - val_acc: 0.6657\n",
      "Epoch 22/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.7280 - acc: 0.6497 - val_loss: 1.5412 - val_acc: 0.6824\n",
      "Epoch 23/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.7235 - acc: 0.6525 - val_loss: 1.5451 - val_acc: 0.6808\n",
      "Epoch 24/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7217 - acc: 0.6540 - val_loss: 1.5809 - val_acc: 0.6702\n",
      "Epoch 25/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7166 - acc: 0.6538 - val_loss: 1.5395 - val_acc: 0.6838\n",
      "Epoch 26/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.7165 - acc: 0.6539 - val_loss: 1.5987 - val_acc: 0.6669\n",
      "Epoch 27/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7067 - acc: 0.6560 - val_loss: 1.5746 - val_acc: 0.6730\n",
      "Epoch 28/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.7047 - acc: 0.6555 - val_loss: 1.5427 - val_acc: 0.6738\n",
      "Epoch 29/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6978 - acc: 0.6585 - val_loss: 1.5961 - val_acc: 0.6698\n",
      "Epoch 30/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6963 - acc: 0.6584 - val_loss: 1.5759 - val_acc: 0.6730\n",
      "Epoch 31/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6967 - acc: 0.6580 - val_loss: 1.5190 - val_acc: 0.6778\n",
      "Epoch 32/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6920 - acc: 0.6589 - val_loss: 1.5125 - val_acc: 0.6798\n",
      "Epoch 33/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6882 - acc: 0.6603 - val_loss: 1.5689 - val_acc: 0.6770\n",
      "Epoch 34/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6896 - acc: 0.6600 - val_loss: 1.5124 - val_acc: 0.6802\n",
      "Epoch 35/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6852 - acc: 0.6605 - val_loss: 1.5542 - val_acc: 0.6736\n",
      "Epoch 36/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6853 - acc: 0.6615 - val_loss: 1.5318 - val_acc: 0.6822\n",
      "Epoch 37/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6803 - acc: 0.6629 - val_loss: 1.5676 - val_acc: 0.6667\n",
      "Epoch 38/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6830 - acc: 0.6614 - val_loss: 1.4977 - val_acc: 0.6913\n",
      "Epoch 39/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6801 - acc: 0.6619 - val_loss: 1.5383 - val_acc: 0.6858\n",
      "Epoch 40/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6732 - acc: 0.6642 - val_loss: 1.4997 - val_acc: 0.6877\n",
      "Epoch 41/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6699 - acc: 0.6636 - val_loss: 1.5204 - val_acc: 0.6772\n",
      "Epoch 42/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6724 - acc: 0.6648 - val_loss: 1.5514 - val_acc: 0.6806\n",
      "Epoch 43/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6625 - acc: 0.6661 - val_loss: 1.5293 - val_acc: 0.6808\n",
      "Epoch 44/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6671 - acc: 0.6655 - val_loss: 1.5110 - val_acc: 0.6792\n",
      "Epoch 45/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6649 - acc: 0.6653 - val_loss: 1.5023 - val_acc: 0.6860\n",
      "Epoch 46/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6685 - acc: 0.6651 - val_loss: 1.5044 - val_acc: 0.6858\n",
      "Epoch 47/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6630 - acc: 0.6646 - val_loss: 1.5244 - val_acc: 0.6762\n",
      "Epoch 48/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6626 - acc: 0.6654 - val_loss: 1.5237 - val_acc: 0.6879\n",
      "Epoch 49/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6624 - acc: 0.6671 - val_loss: 1.4987 - val_acc: 0.6925\n",
      "Epoch 50/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6588 - acc: 0.6665 - val_loss: 1.4558 - val_acc: 0.6941\n",
      "Epoch 51/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6590 - acc: 0.6668 - val_loss: 1.5527 - val_acc: 0.6730\n",
      "Epoch 52/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6563 - acc: 0.6678 - val_loss: 1.5570 - val_acc: 0.6728\n",
      "Epoch 53/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6549 - acc: 0.6675 - val_loss: 1.4718 - val_acc: 0.6931\n",
      "Epoch 54/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6538 - acc: 0.6673 - val_loss: 1.4726 - val_acc: 0.6909\n",
      "Epoch 55/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6552 - acc: 0.6673 - val_loss: 1.5078 - val_acc: 0.6786\n",
      "Epoch 56/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6540 - acc: 0.6682 - val_loss: 1.5419 - val_acc: 0.6854\n",
      "Epoch 57/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6513 - acc: 0.6687 - val_loss: 1.4896 - val_acc: 0.6874\n",
      "Epoch 58/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6477 - acc: 0.6687 - val_loss: 1.4907 - val_acc: 0.6901\n",
      "Epoch 59/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6507 - acc: 0.6681 - val_loss: 1.5080 - val_acc: 0.6810\n",
      "Epoch 60/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6466 - acc: 0.6700 - val_loss: 1.5010 - val_acc: 0.6868\n",
      "Epoch 61/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6426 - acc: 0.6698 - val_loss: 1.5190 - val_acc: 0.6814\n",
      "Epoch 62/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6433 - acc: 0.6679 - val_loss: 1.4552 - val_acc: 0.6911\n",
      "Epoch 63/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6436 - acc: 0.6686 - val_loss: 1.4593 - val_acc: 0.6961\n",
      "Epoch 64/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6471 - acc: 0.6668 - val_loss: 1.4856 - val_acc: 0.6856\n",
      "Epoch 65/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6404 - acc: 0.6701 - val_loss: 1.5012 - val_acc: 0.6828\n",
      "Epoch 66/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6493 - acc: 0.6701 - val_loss: 1.5287 - val_acc: 0.6810\n",
      "Epoch 67/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6382 - acc: 0.6700 - val_loss: 1.4822 - val_acc: 0.6921\n",
      "Epoch 68/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6433 - acc: 0.6716 - val_loss: 1.4980 - val_acc: 0.6856\n",
      "Epoch 69/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6409 - acc: 0.6707 - val_loss: 1.5019 - val_acc: 0.6842\n",
      "Epoch 70/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6435 - acc: 0.6698 - val_loss: 1.4889 - val_acc: 0.6860\n",
      "Epoch 71/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6356 - acc: 0.6721 - val_loss: 1.5016 - val_acc: 0.6824\n",
      "Epoch 72/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6352 - acc: 0.6719 - val_loss: 1.4908 - val_acc: 0.6905\n",
      "Epoch 73/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6377 - acc: 0.6705 - val_loss: 1.4512 - val_acc: 0.6995\n",
      "Epoch 74/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6362 - acc: 0.6716 - val_loss: 1.4881 - val_acc: 0.6866\n",
      "Epoch 75/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6373 - acc: 0.6718 - val_loss: 1.4495 - val_acc: 0.6949\n",
      "Epoch 76/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6328 - acc: 0.6717 - val_loss: 1.5240 - val_acc: 0.6840\n",
      "Epoch 77/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6315 - acc: 0.6719 - val_loss: 1.4671 - val_acc: 0.6893\n",
      "Epoch 78/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6360 - acc: 0.6709 - val_loss: 1.4941 - val_acc: 0.6879\n",
      "Epoch 79/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6334 - acc: 0.6723 - val_loss: 1.4449 - val_acc: 0.7003\n",
      "Epoch 80/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6320 - acc: 0.6732 - val_loss: 1.4827 - val_acc: 0.6933\n",
      "Epoch 81/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6305 - acc: 0.6735 - val_loss: 1.4739 - val_acc: 0.6943\n",
      "Epoch 82/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6265 - acc: 0.6728 - val_loss: 1.5251 - val_acc: 0.6816\n",
      "Epoch 83/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6297 - acc: 0.6722 - val_loss: 1.4708 - val_acc: 0.6933\n",
      "Epoch 84/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6299 - acc: 0.6719 - val_loss: 1.4592 - val_acc: 0.6925\n",
      "Epoch 85/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6298 - acc: 0.6733 - val_loss: 1.4829 - val_acc: 0.6885\n",
      "Epoch 86/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6223 - acc: 0.6737 - val_loss: 1.4843 - val_acc: 0.6919\n",
      "Epoch 87/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6230 - acc: 0.6734 - val_loss: 1.4719 - val_acc: 0.6973\n",
      "Epoch 88/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6203 - acc: 0.6743 - val_loss: 1.4495 - val_acc: 0.6979\n",
      "Epoch 89/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6267 - acc: 0.6741 - val_loss: 1.4335 - val_acc: 0.6995\n",
      "Epoch 90/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6221 - acc: 0.6732 - val_loss: 1.5132 - val_acc: 0.6790\n",
      "Epoch 91/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6249 - acc: 0.6746 - val_loss: 1.4980 - val_acc: 0.6929\n",
      "Epoch 92/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6203 - acc: 0.6734 - val_loss: 1.5037 - val_acc: 0.6870\n",
      "Epoch 93/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6225 - acc: 0.6736 - val_loss: 1.4961 - val_acc: 0.6881\n",
      "Epoch 94/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6215 - acc: 0.6749 - val_loss: 1.4695 - val_acc: 0.6927\n",
      "Epoch 95/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6213 - acc: 0.6723 - val_loss: 1.4485 - val_acc: 0.6969\n",
      "Epoch 96/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6211 - acc: 0.6739 - val_loss: 1.4777 - val_acc: 0.6881\n",
      "Epoch 97/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6262 - acc: 0.6729 - val_loss: 1.5315 - val_acc: 0.6774\n",
      "Epoch 98/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6224 - acc: 0.6745 - val_loss: 1.5267 - val_acc: 0.6820\n",
      "Epoch 99/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6188 - acc: 0.6746 - val_loss: 1.4723 - val_acc: 0.6854\n",
      "Epoch 100/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6189 - acc: 0.6752 - val_loss: 1.4733 - val_acc: 0.6925\n",
      "Epoch 101/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6178 - acc: 0.6754 - val_loss: 1.3856 - val_acc: 0.7108\n",
      "Epoch 102/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6179 - acc: 0.6750 - val_loss: 1.4693 - val_acc: 0.6957\n",
      "Epoch 103/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6242 - acc: 0.6734 - val_loss: 1.4854 - val_acc: 0.6895\n",
      "Epoch 104/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6200 - acc: 0.6744 - val_loss: 1.4551 - val_acc: 0.6923\n",
      "Epoch 105/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6170 - acc: 0.6746 - val_loss: 1.4402 - val_acc: 0.6985\n",
      "Epoch 106/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6136 - acc: 0.6750 - val_loss: 1.4603 - val_acc: 0.7027\n",
      "Epoch 107/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6149 - acc: 0.6760 - val_loss: 1.4370 - val_acc: 0.6975\n",
      "Epoch 108/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6165 - acc: 0.6754 - val_loss: 1.4697 - val_acc: 0.6957\n",
      "Epoch 109/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6112 - acc: 0.6742 - val_loss: 1.4977 - val_acc: 0.6899\n",
      "Epoch 110/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6142 - acc: 0.6757 - val_loss: 1.4613 - val_acc: 0.6933\n",
      "Epoch 111/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6136 - acc: 0.6751 - val_loss: 1.4499 - val_acc: 0.6973\n",
      "Epoch 112/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6120 - acc: 0.6746 - val_loss: 1.4331 - val_acc: 0.6999\n",
      "Epoch 113/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6140 - acc: 0.6746 - val_loss: 1.4715 - val_acc: 0.6854\n",
      "Epoch 114/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6132 - acc: 0.6761 - val_loss: 1.4472 - val_acc: 0.6953\n",
      "Epoch 115/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6151 - acc: 0.6760 - val_loss: 1.4368 - val_acc: 0.7011\n",
      "Epoch 116/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6124 - acc: 0.6747 - val_loss: 1.4779 - val_acc: 0.6943\n",
      "Epoch 117/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6103 - acc: 0.6743 - val_loss: 1.4324 - val_acc: 0.6979\n",
      "Epoch 118/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6088 - acc: 0.6762 - val_loss: 1.4625 - val_acc: 0.6903\n",
      "Epoch 119/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6110 - acc: 0.6767 - val_loss: 1.4628 - val_acc: 0.6955\n",
      "Epoch 120/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6054 - acc: 0.6754 - val_loss: 1.4440 - val_acc: 0.6965\n",
      "Epoch 121/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6087 - acc: 0.6756 - val_loss: 1.4792 - val_acc: 0.6947\n",
      "Epoch 122/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6121 - acc: 0.6761 - val_loss: 1.4659 - val_acc: 0.7017\n",
      "Epoch 123/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6114 - acc: 0.6761 - val_loss: 1.4702 - val_acc: 0.6907\n",
      "Epoch 124/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6081 - acc: 0.6763 - val_loss: 1.4518 - val_acc: 0.6973\n",
      "Epoch 125/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6039 - acc: 0.6757 - val_loss: 1.4590 - val_acc: 0.6895\n",
      "Epoch 126/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6071 - acc: 0.6762 - val_loss: 1.4091 - val_acc: 0.7027\n",
      "Epoch 127/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6059 - acc: 0.6768 - val_loss: 1.4767 - val_acc: 0.6973\n",
      "Epoch 128/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6089 - acc: 0.6766 - val_loss: 1.4810 - val_acc: 0.6874\n",
      "Epoch 129/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6104 - acc: 0.6775 - val_loss: 1.4608 - val_acc: 0.6899\n",
      "Epoch 130/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6079 - acc: 0.6772 - val_loss: 1.4112 - val_acc: 0.7041\n",
      "Epoch 131/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6072 - acc: 0.6767 - val_loss: 1.4125 - val_acc: 0.7043\n",
      "Epoch 132/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6072 - acc: 0.6768 - val_loss: 1.4812 - val_acc: 0.6840\n",
      "Epoch 133/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6067 - acc: 0.6758 - val_loss: 1.4780 - val_acc: 0.6893\n",
      "Epoch 134/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6081 - acc: 0.6760 - val_loss: 1.4308 - val_acc: 0.6963\n",
      "Epoch 135/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6078 - acc: 0.6773 - val_loss: 1.4514 - val_acc: 0.6967\n",
      "Epoch 136/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6039 - acc: 0.6785 - val_loss: 1.4698 - val_acc: 0.6901\n",
      "Epoch 137/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6026 - acc: 0.6766 - val_loss: 1.4774 - val_acc: 0.6883\n",
      "Epoch 138/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6108 - acc: 0.6759 - val_loss: 1.4671 - val_acc: 0.6911\n",
      "Epoch 139/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6084 - acc: 0.6764 - val_loss: 1.4801 - val_acc: 0.6919\n",
      "Epoch 140/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6057 - acc: 0.6753 - val_loss: 1.4635 - val_acc: 0.6915\n",
      "Epoch 141/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6047 - acc: 0.6769 - val_loss: 1.4327 - val_acc: 0.7039\n",
      "Epoch 142/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6048 - acc: 0.6778 - val_loss: 1.3865 - val_acc: 0.7086\n",
      "Epoch 143/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6026 - acc: 0.6775 - val_loss: 1.4340 - val_acc: 0.6981\n",
      "Epoch 144/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6063 - acc: 0.6763 - val_loss: 1.4651 - val_acc: 0.6927\n",
      "Epoch 145/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6024 - acc: 0.6767 - val_loss: 1.4231 - val_acc: 0.7056\n",
      "Epoch 146/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6011 - acc: 0.6783 - val_loss: 1.4799 - val_acc: 0.6868\n",
      "Epoch 147/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6006 - acc: 0.6773 - val_loss: 1.4203 - val_acc: 0.7009\n",
      "Epoch 148/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6047 - acc: 0.6765 - val_loss: 1.4454 - val_acc: 0.6963\n",
      "Epoch 149/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5988 - acc: 0.6775 - val_loss: 1.4466 - val_acc: 0.6963\n",
      "Epoch 150/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5988 - acc: 0.6776 - val_loss: 1.4741 - val_acc: 0.6967\n",
      "Epoch 151/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5997 - acc: 0.6757 - val_loss: 1.4100 - val_acc: 0.7066\n",
      "Epoch 152/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6071 - acc: 0.6757 - val_loss: 1.4512 - val_acc: 0.7003\n",
      "Epoch 153/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6025 - acc: 0.6768 - val_loss: 1.4637 - val_acc: 0.6943\n",
      "Epoch 154/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6002 - acc: 0.6774 - val_loss: 1.4657 - val_acc: 0.6872\n",
      "Epoch 155/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6033 - acc: 0.6774 - val_loss: 1.4572 - val_acc: 0.6929\n",
      "Epoch 156/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6033 - acc: 0.6763 - val_loss: 1.4252 - val_acc: 0.6979\n",
      "Epoch 157/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5992 - acc: 0.6768 - val_loss: 1.4527 - val_acc: 0.6919\n",
      "Epoch 158/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5993 - acc: 0.6788 - val_loss: 1.4539 - val_acc: 0.6957\n",
      "Epoch 159/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6074 - acc: 0.6766 - val_loss: 1.4612 - val_acc: 0.6977\n",
      "Epoch 160/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5998 - acc: 0.6778 - val_loss: 1.3945 - val_acc: 0.7082\n",
      "Epoch 161/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5975 - acc: 0.6778 - val_loss: 1.4585 - val_acc: 0.6949\n",
      "Epoch 162/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6038 - acc: 0.6771 - val_loss: 1.4623 - val_acc: 0.6939\n",
      "Epoch 163/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6044 - acc: 0.6776 - val_loss: 1.4543 - val_acc: 0.6874\n",
      "Epoch 164/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5968 - acc: 0.6769 - val_loss: 1.4548 - val_acc: 0.6921\n",
      "Epoch 165/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6016 - acc: 0.6781 - val_loss: 1.4597 - val_acc: 0.6949\n",
      "Epoch 166/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5974 - acc: 0.6792 - val_loss: 1.4606 - val_acc: 0.6961\n",
      "Epoch 167/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6000 - acc: 0.6776 - val_loss: 1.4540 - val_acc: 0.6949\n",
      "Epoch 168/200\n",
      "50282/50282 [==============================] - 76s - loss: 1.5981 - acc: 0.6775 - val_loss: 1.4733 - val_acc: 0.6903\n",
      "Epoch 169/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5996 - acc: 0.6776 - val_loss: 1.4313 - val_acc: 0.7019\n",
      "Epoch 170/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.6035 - acc: 0.6769 - val_loss: 1.4550 - val_acc: 0.6965\n",
      "Epoch 171/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5969 - acc: 0.6781 - val_loss: 1.3881 - val_acc: 0.7039\n",
      "Epoch 172/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6009 - acc: 0.6776 - val_loss: 1.4815 - val_acc: 0.6911\n",
      "Epoch 173/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6018 - acc: 0.6769 - val_loss: 1.4107 - val_acc: 0.6979\n",
      "Epoch 174/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5954 - acc: 0.6792 - val_loss: 1.4717 - val_acc: 0.6927\n",
      "Epoch 175/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5969 - acc: 0.6783 - val_loss: 1.4168 - val_acc: 0.7019\n",
      "Epoch 176/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5952 - acc: 0.6781 - val_loss: 1.4189 - val_acc: 0.7007\n",
      "Epoch 177/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5925 - acc: 0.6781 - val_loss: 1.4684 - val_acc: 0.6925\n",
      "Epoch 178/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5993 - acc: 0.6772 - val_loss: 1.4711 - val_acc: 0.6975\n",
      "Epoch 179/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5992 - acc: 0.6782 - val_loss: 1.4658 - val_acc: 0.6959\n",
      "Epoch 180/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5971 - acc: 0.6771 - val_loss: 1.4097 - val_acc: 0.7013\n",
      "Epoch 181/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5984 - acc: 0.6786 - val_loss: 1.4019 - val_acc: 0.7047\n",
      "Epoch 182/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5966 - acc: 0.6792 - val_loss: 1.4220 - val_acc: 0.7009\n",
      "Epoch 183/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5983 - acc: 0.6776 - val_loss: 1.3880 - val_acc: 0.7106\n",
      "Epoch 184/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5911 - acc: 0.6777 - val_loss: 1.4561 - val_acc: 0.6923\n",
      "Epoch 185/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.6000 - acc: 0.6788 - val_loss: 1.4597 - val_acc: 0.6939\n",
      "Epoch 186/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5942 - acc: 0.6781 - val_loss: 1.4302 - val_acc: 0.6965\n",
      "Epoch 187/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5942 - acc: 0.6788 - val_loss: 1.4554 - val_acc: 0.6927\n",
      "Epoch 188/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5951 - acc: 0.6793 - val_loss: 1.4563 - val_acc: 0.6961\n",
      "Epoch 189/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5973 - acc: 0.6773 - val_loss: 1.4600 - val_acc: 0.6953\n",
      "Epoch 190/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5990 - acc: 0.6784 - val_loss: 1.3900 - val_acc: 0.7064\n",
      "Epoch 191/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5988 - acc: 0.6773 - val_loss: 1.4798 - val_acc: 0.6975\n",
      "Epoch 192/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5981 - acc: 0.6779 - val_loss: 1.4631 - val_acc: 0.6947\n",
      "Epoch 193/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5992 - acc: 0.6770 - val_loss: 1.3682 - val_acc: 0.7106\n",
      "Epoch 194/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5916 - acc: 0.6773 - val_loss: 1.3875 - val_acc: 0.7074\n",
      "Epoch 195/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5937 - acc: 0.6776 - val_loss: 1.4886 - val_acc: 0.6937\n",
      "Epoch 196/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5927 - acc: 0.6779 - val_loss: 1.4773 - val_acc: 0.6887\n",
      "Epoch 197/200\n",
      "50282/50282 [==============================] - 75s - loss: 1.5972 - acc: 0.6778 - val_loss: 1.3918 - val_acc: 0.7156\n",
      "Epoch 198/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5927 - acc: 0.6783 - val_loss: 1.4595 - val_acc: 0.6891\n",
      "Epoch 199/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5917 - acc: 0.6778 - val_loss: 1.4583 - val_acc: 0.6937\n",
      "Epoch 200/200\n",
      "50282/50282 [==============================] - 74s - loss: 1.5917 - acc: 0.6784 - val_loss: 1.4217 - val_acc: 0.6991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x125132c50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model fit from data served from the generator in batches of 128, validated on another random 5000 or so \n",
    "# observations served from generator.\n",
    "model.fit_generator(generator = datagen.flow(X_data, Y_target, batch_size=batch_size),\n",
    "                    samples_per_epoch = len(X_data),\n",
    "                    nb_epoch = nb_epoch,\n",
    "                    validation_data = datagen.flow(X_data, Y_target, batch_size=int(len(X_data)/10)),\n",
    "                    nb_val_samples = 1,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/test.csv', 'rb') as csvfile:\n",
    "    testing_data = np.loadtxt(csvfile, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_bitmaps = testing_data[:, 9:439].reshape(testing_data.shape[0], 1, img_rows, img_cols)\n",
    "test_bitmaps /= 255\n",
    "test_ids = testing_data[:, 0]\n",
    "test_bitmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predict = np_utils.categorical_probas_to_classes(model.predict(test_bitmaps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = np.column_stack((test_ids, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join('Convolutional Neural Network', 'kerasCNN_Preprocessed.csv')\n",
    "np.savetxt('kerasCNN_Preprocessed.csv', output, fmt='%d', header=\"Id,Character\", delimiter=\",\", comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join('Convolutional Neural Network', 'model.h5')\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
